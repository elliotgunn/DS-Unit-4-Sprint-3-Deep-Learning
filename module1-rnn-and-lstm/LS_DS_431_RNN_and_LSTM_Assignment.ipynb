{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!\n",
    "\n",
    "Resource: \n",
    "- https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 5584464\n"
     ]
    }
   ],
   "source": [
    "with open('./data/shakespeare.txt', encoding='utf-8') as file:\n",
    "  text = file.read().lower()\n",
    "print('text length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "project gutenberg’s the complete works of william shakespeare, by william\n",
      "shakespeare\n",
      "\n",
      "this ebook is for the use of anyone anywhere in the united states and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  you m\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  79\n",
      "txt_data_size :  5584464\n"
     ]
    }
   ],
   "source": [
    "# get unique chars only\n",
    "\n",
    "import numpy as np\n",
    "chars = list(set(text))\n",
    "\n",
    "num_chars = len(chars)\n",
    "txt_data_size = len(text)\n",
    "\n",
    "print(\"unique characters : \", num_chars)\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p': 0, '8': 1, '0': 2, 'w': 3, '\"': 4, '1': 5, '[': 6, 'ê': 7, \"'\": 8, '!': 9, '\\\\': 10, 'c': 11, '%': 12, 'v': 13, 't': 14, 'ç': 15, 'y': 16, ')': 17, ']': 18, 's': 19, 'é': 20, 'j': 21, '’': 22, 'd': 23, '”': 24, '`': 25, 'm': 26, '#': 27, '4': 28, '.': 29, 'è': 30, 'b': 31, '‘': 32, 'q': 33, 'e': 34, '(': 35, '*': 36, '—': 37, 'â': 38, 'œ': 39, 'l': 40, 'o': 41, 'g': 42, '\\ufeff': 43, '_': 44, '/': 45, ':': 46, '7': 47, 'n': 48, '-': 49, 'z': 50, ',': 51, '&': 52, ';': 53, '5': 54, 'f': 55, '3': 56, 'u': 57, 'a': 58, 'î': 59, '\\t': 60, '@': 61, '6': 62, ' ': 63, 'i': 64, 'r': 65, '$': 66, 'æ': 67, '}': 68, 'k': 69, '?': 70, '|': 71, 'h': 72, '2': 73, '“': 74, '9': 75, 'à': 76, 'x': 77, '\\n': 78}\n",
      "----------------------------------------------------\n",
      "{0: 'p', 1: '8', 2: '0', 3: 'w', 4: '\"', 5: '1', 6: '[', 7: 'ê', 8: \"'\", 9: '!', 10: '\\\\', 11: 'c', 12: '%', 13: 'v', 14: 't', 15: 'ç', 16: 'y', 17: ')', 18: ']', 19: 's', 20: 'é', 21: 'j', 22: '’', 23: 'd', 24: '”', 25: '`', 26: 'm', 27: '#', 28: '4', 29: '.', 30: 'è', 31: 'b', 32: '‘', 33: 'q', 34: 'e', 35: '(', 36: '*', 37: '—', 38: 'â', 39: 'œ', 40: 'l', 41: 'o', 42: 'g', 43: '\\ufeff', 44: '_', 45: '/', 46: ':', 47: '7', 48: 'n', 49: '-', 50: 'z', 51: ',', 52: '&', 53: ';', 54: '5', 55: 'f', 56: '3', 57: 'u', 58: 'a', 59: 'î', 60: '\\t', 61: '@', 62: '6', 63: ' ', 64: 'i', 65: 'r', 66: '$', 67: 'æ', 68: '}', 69: 'k', 70: '?', 71: '|', 72: 'h', 73: '2', 74: '“', 75: '9', 76: 'à', 77: 'x', 78: '\\n'}\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# before training, we need to map strings to a numerical rep\n",
    "# create two dicts/tables \n",
    "# one maping characters to numbers, \n",
    "\n",
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patters: 5584364\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# we must split the data up into subseqences with a set length\n",
    "\n",
    "# this sets the max num of chars for each subsequence\n",
    "# recall: txt_data_size already determined\n",
    "seq_length = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, txt_data_size - seq_length, 1): # 0, 5584464 - 100 , 1\n",
    "  seq_in = text[i:i + seq_length] # 0:0 + 100\n",
    "  seq_out = text[i + seq_length] # 0 + 100\n",
    "  \n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "\n",
    "  \n",
    "# this now tells us that we have X number of training patterns\n",
    "n_patterns = len(dataX)\n",
    "print('Total patters:', n_patterns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that our training data is prepared\n",
    "# we have to transform it so it is suitable for use with Keras\n",
    "\n",
    "\n",
    "# 1. reshape X\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. normalize to make it easier\n",
    "# makes the patterns easier to learn by the LSTM network that uses \n",
    "# the sigmoid activation function by default.\n",
    "X = X / float(num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# 3. one hot encode the output variable\n",
    "# to predict the probability of each of the unique chars\n",
    "# rather than force it to predict pricisely the next character\n",
    "y = np_utils.to_categorical(dataY)\n",
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/envs/U4-S3-DNN/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 79)                10191     \n",
      "=================================================================\n",
      "Total params: 76,751\n",
      "Trainable params: 76,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the LSTM model using the ENTIRE dataset\n",
    "\n",
    "# couple things:\n",
    "# - we defined a single hidden LSTM layer with 128 memory units\n",
    "# - dropout prob of 20\n",
    "# - output is a Dense layer, activation is softmax to ouput a prob pred for each of the unique chars\n",
    "\n",
    "# think of the problem as a single char classification problem\n",
    "# with [unique chars] number of classes\n",
    "# as such, optimizing the log loss here using Adam for speed\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data\n",
    "history = model.fit(X, y, epochs=5, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 1607913\n",
      "Pattern: [63, 64, 26, 0, 41, 65, 14, 29, 78, 63, 63, 63, 63, 14, 72, 34, 63, 23, 58, 57, 0, 72, 64, 48, 63, 11, 72, 58, 65, 40, 34, 19, 63, 64, 19, 63, 11, 65, 41, 3, 48, 34, 23, 63, 69, 64, 48, 42, 63, 64, 48, 63, 65, 72, 34, 64, 26, 19, 53, 78, 63, 63, 63, 63, 14, 72, 34, 63, 31, 58, 19, 14, 58, 65, 23, 63, 41, 55, 63, 41, 65, 40, 34, 58, 48, 19, 63, 3, 64, 14, 72, 63, 72, 64, 26, 63, 64, 19, 63, 21]\n",
      "Seed sequence: \"  import.\n",
      "    the dauphin charles is crowned king in rheims;\n",
      "    the bastard of orleans with him is j \"\n"
     ]
    }
   ],
   "source": [
    "# 1. pick a random starting subsequence as input\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "print('Start:', start)\n",
    "print('Pattern:', pattern)\n",
    "print('Seed sequence:', \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'i', 'm', 'p', 'o', 'r', 't', '.', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', ' ', 'd', 'a', 'u', 'p', 'h', 'i', 'n', ' ', 'c', 'h', 'a', 'r', 'l', 'e', 's', ' ', 'i', 's', ' ', 'c', 'r', 'o', 'w', 'n', 'e', 'd', ' ', 'k', 'i', 'n', 'g', ' ', 'i', 'n', ' ', 'r', 'h', 'e', 'i', 'm', 's', ';', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 't', 'a', 'r', 'd', ' ', 'o', 'f', ' ', 'o', 'r', 'l', 'e', 'a', 'n', 's', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'i', 'm', ' ', 'i', 's', ' ', 'j']\n"
     ]
    }
   ],
   "source": [
    "# create output with the starting seed sequence\n",
    "out = [int_to_char[value] for value in pattern]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 2. then generate the next char\n",
    "# then update the seed seq to add the generated char to the end\n",
    "# delete the first char\n",
    "\n",
    "# basically, putting all the above code together into a loop\n",
    "import sys\n",
    "\n",
    "for i in range(1000):\n",
    "    # transform for use in Keras & normalize\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(num_chars)\n",
    "    \n",
    "    # predict & get index location as int\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    \n",
    "    # convert the index int to a char\n",
    "    result = int_to_char[index]\n",
    "    \n",
    "    # continue along seq in\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    \n",
    "    # append the predicted char to the output/generated text\n",
    "    out.append(result)\n",
    "    \n",
    "    # update pattern\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: [' ', 'i', 'm', 'p', 'o', 'r', 't', '.', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', ' ', 'd', 'a', 'u', 'p', 'h', 'i', 'n', ' ', 'c', 'h', 'a', 'r', 'l', 'e', 's', ' ', 'i', 's', ' ', 'c', 'r', 'o', 'w', 'n', 'e', 'd', ' ', 'k', 'i', 'n', 'g', ' ', 'i', 'n', ' ', 'r', 'h', 'e', 'i', 'm', 's', ';', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 't', 'a', 'r', 'd', ' ', 'o', 'f', ' ', 'o', 'r', 'l', 'e', 'a', 'n', 's', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'i', 'm', ' ', 'i', 's', ' ', 'j', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "print('Generated Text:', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S3-DNN (Python3)",
   "language": "python",
   "name": "u4-s3-dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
