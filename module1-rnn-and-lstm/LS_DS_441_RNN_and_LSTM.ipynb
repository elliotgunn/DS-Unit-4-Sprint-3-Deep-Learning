{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LS_DS_441_RNN_and_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "## _aka_ PREDICTING THE FUTURE!\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ti23G0gRe3kr",
        "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 16s 1us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Build model...\n",
            "WARNING:tensorflow:From /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train...\n",
            "WARNING:tensorflow:From /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 [==============================] - 96s 4ms/step - loss: 0.4567 - acc: 0.7836 - val_loss: 0.3793 - val_acc: 0.8348\n",
            "Epoch 2/15\n",
            "25000/25000 [==============================] - 93s 4ms/step - loss: 0.2977 - acc: 0.8785 - val_loss: 0.3773 - val_acc: 0.8362\n",
            "Epoch 3/15\n",
            "25000/25000 [==============================] - 91s 4ms/step - loss: 0.2182 - acc: 0.9148 - val_loss: 0.4150 - val_acc: 0.8340\n",
            "Epoch 4/15\n",
            "25000/25000 [==============================] - 90s 4ms/step - loss: 0.1570 - acc: 0.9430 - val_loss: 0.4670 - val_acc: 0.8300\n",
            "Epoch 5/15\n",
            "25000/25000 [==============================] - 92s 4ms/step - loss: 0.1108 - acc: 0.9595 - val_loss: 0.6126 - val_acc: 0.8107\n",
            "Epoch 6/15\n",
            "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0802 - acc: 0.9712 - val_loss: 0.6611 - val_acc: 0.8190\n",
            "Epoch 7/15\n",
            "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0674 - acc: 0.9765 - val_loss: 0.7836 - val_acc: 0.8056\n",
            "Epoch 8/15\n",
            "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0412 - acc: 0.9865 - val_loss: 0.7688 - val_acc: 0.8193\n",
            "Epoch 9/15\n",
            "25000/25000 [==============================] - 94s 4ms/step - loss: 0.0301 - acc: 0.9904 - val_loss: 0.9114 - val_acc: 0.8117\n",
            "Epoch 10/15\n",
            "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0274 - acc: 0.9911 - val_loss: 0.8979 - val_acc: 0.8143\n",
            "Epoch 11/15\n",
            "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0174 - acc: 0.9943 - val_loss: 1.0019 - val_acc: 0.8130\n",
            "Epoch 12/15\n",
            "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.9923 - val_acc: 0.8082\n",
            "Epoch 13/15\n",
            "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0111 - acc: 0.9968 - val_loss: 1.1199 - val_acc: 0.8141\n",
            "Epoch 14/15\n",
            "25000/25000 [==============================] - 94s 4ms/step - loss: 0.0090 - acc: 0.9975 - val_loss: 1.1868 - val_acc: 0.8142\n",
            "Epoch 15/15\n",
            "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0132 - acc: 0.9959 - val_loss: 1.0503 - val_acc: 0.8050\n",
            "25000/25000 [==============================] - 10s 394us/step\n",
            "Test score: 1.0503065874004365\n",
            "Test accuracy: 0.80496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pETWPIe362y"
      },
      "source": [
        "### RNN Text generation with NumPy\n",
        "\n",
        "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fz1m55G5WSrQ"
      },
      "source": [
        "#### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ahlHBeoZCaLX",
        "outputId": "1e286481-b044-4430-a78d-23ed389f0a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.5.3)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/90/18ac0e5340b6228c25cc8e79835c3811e7553b2b9ae87296dfeb62b7866d/tldextract-2.2.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 21.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 26.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.3.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 41.2MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (41.2.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.3.0->newspaper3k) (0.46)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, feedparser\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=c37d9be1e8c5fe9595649829baa52d6c0bcd96a1244fb145613abb56ec8d55a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3356 sha256=29afd2c00cf64e3b6b8fc2d3fa4839525075f094f42bc7b41690dd0647d0c929\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398407 sha256=e299b98bd1787d4825fb1b259ea81c9173172dd5456c45b6de9ca482b6bca09f\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=ca9db498c95eb9376b7457e7388a2092392361c99cfd67c18e69f8561e530e1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k feedparser\n",
            "Installing collected packages: tinysegmenter, requests-file, tldextract, feedfinder2, jieba3k, feedparser, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.4.3 tinysegmenter-0.3 tldextract-2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fTPlziljCiNJ",
        "colab": {}
      },
      "source": [
        "import newspaper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bk9JF2zaCxoO",
        "outputId": "ca27f552-49d1-484e-b6d9-5444bafed7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ap = newspaper.build('https://www.apnews.com')\n",
        "len(ap.articles)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vc6JgAIJDF4E",
        "colab": {}
      },
      "source": [
        "article_text = ''\n",
        "\n",
        "for article in ap.articles[:1]:\n",
        "  try:\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article_text += '\\n\\n' + article.text\n",
        "  except:\n",
        "    print('Failed: ' + article.url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fByWxHXoJTDe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "29b01feb-94f5-4400-add1-33b3ba3feaa7"
      },
      "source": [
        "article_text"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nLibya is spiraling toward its third civil war in less than a decade, creating a foreign policy crisis for the Trump administration in a strategic oil-rich corner of North Africa amid rising fears that jihadi terrorist groups are once again exploiting the power vacuum opened by renewed violence in recent months.\\n\\nThe administration has spent the past two years trying to avoid getting sucked into the vortex, apparently hoping anti-jihadi rebel leader Khalifa Haftar — a former Libyan army colonel with alleged CIA ties — would topple the weak, internationally recognized government in Tripoli that analysts say is protected by radical militias.\\n\\nBut with Mr. Haftar’s Libyan National Army now mired in its months-old push to capture Tripoli, there are signs that the White House is shifting posture, backing a cease-fire and reviving a U.N. peace process aimed at reaching a deal between Mr. Haftar’s forces and those upholding the government in Tripoli.\\n\\n“The situation is presenting a complex challenge for the Trump administration,” said a source close to American intelligence on Libya, who pointed to a little-reported joint statement that U.S. officials signed in mid-July with France, Britain, Egypt, Italy and the United Arab Emirates backing an immediate cease-fire.\\n\\nThe statement, which said “there can be no military solution in Libya” and warned of “terror groups” exploiting the political void, was issued just months after President Trump unexpectedly reached out to Mr. Haftar in a phone call from the Oval Office to praise the military leader and his forces.\\n\\nDuring the call in April, Mr. Trump said he “recognized [Mr. Haftar‘s] significant role in fighting terrorism and securing Libya’s oil resources.” The two also “discussed a shared vision for Libya’s transition to a stable, democratic political system,” the White House said in a statement after the call.\\n\\nAt the time, the administration was refusing to support a U.N. Security Council resolution demanding a cease-fire and an end to Mr. Haftar’s campaign on the Tripoli-based Government of National Accord.\\n\\nBecause the call was made public, U.S. officials are downplaying the notion that a major U.S. policy shift is in the works.\\n\\nThe White House and the State Department declined to comment on the confused state of play in Libya. But one U.S. official, who spoke on the condition on anonymity, asserted that “this is not a policy shift — we weren’t supportive of the cease-fire statement several months ago because we just didn’t think it was the right time.”\\n\\n“Our position on Haftar has remained constant,” the official added. “Certainly he has to be part of a solution. The goal here is to get to elections in Libya.”\\n\\nA State Department official told The Washington Times on background that the U.S. believes that “lasting peace and stability in Libya will only come through a political solution.”\\n\\n“We call on all parties to rapidly return to U.N. political mediation, the success of which depends upon a cease-fire in and around Tripoli,” the official said.\\n\\nAn enigmatic figure, the 75-year-old Mr. Haftar has become famous in national security circles over the past three years based on his background. He was reportedly once close to Libyan dictator Moammar Gadhafi and played a key role in the coup that brought the obscure army colonel to power in 1969.\\n\\nMr. Haftar is believed to have had a falling-out with Mr. Gadhafi sometime in the late 1990s, at which point he moved to the U.S. and was the subject of unconfirmed media reports that he cultivated ties with the CIA.\\n\\nSecurity crisis\\n\\nHow the situation in Libya ultimately plays out could have major implications for global oil markets. The country has some of the world’s biggest proven reserves, but production has plummeted since Gadhafi was ousted and killed in 2011.\\n\\nMilitant attacks on oil infrastructure in recent years sent production plummeting to less than 300,000 barrels per day, a fifth of the output in 2010, when Libya was a formidable player in the OPEC oil cartel.\\n\\nThe lost revenue, coupled with a scramble to control what production was continuing, has left the government in Tripoli bankrupt. The predicament is compounded by Libya’s popularity as a route for migrants from other African and Middle Eastern countries seeking a passage to Europe.\\n\\nThe flood of refugees from Libya and the proliferation of smugglers have sparked political and humanitarian crises in European countries. On Friday, Libyan coast guard officials said they had recovered dozens of bodies of Europe-bound migrants as search operations continued. Up to 150 people, including women and children, are believed to have died when their boats capsized in rough Mediterranean waters.\\n\\nA recent article by the German magazine Der Spiegel said many migrants are interned in brutal camps, kept in slavelike dependency and in some cases conscripted into military service. More than 50 African migrants were killed when Mr. Haftar’s forces bombed a weapons facility beside a major refugee camp outside Tripoli on July 3.\\n\\nThen there is the threat of jihadis, who analysts say are exploiting the lack of a functioning government inside Libya.\\n\\n“If fighting between Haftar’s forces and militias loyal to [the Tripoli government] continues indefinitely, the problem of regional terrorists like Islamic State, al Qaeda and other groups being able to feed off the instability won’t go away,” said Bill Roggio, an editor of the Long War Journal at the Foundation for Defense of Democracies think tank in Washington.\\n\\n“The instability allows these groups to recruit, train, obtain and smuggle weapons and attack weak points in the country and in other countries nearby,” Mr. Roggio said. “If the two main Libyan factions weren’t fighting each other the way they are right now, they’d have greater resources to take on the terrorist groups that still operate there.”\\n\\nGeopolitical mess\\n\\nChurning beneath the chaos is a deeper geopolitical fight that has drawn a number of outside countries into the Libyan power struggle.\\n\\nEgypt and the UAE are backing Mr. Haftar as a secular force with the aim of crushing any chance that Islamists will have a government role. Other countries, led by Turkey and Qatar, back the Government of National Accord, headed by Fayez Mustafa al-Sarraj and more tolerant of Islamist political factions.\\n\\nThe U.S. official who spoke anonymously with The Times said it was particularly significant that Egypt and the UAE had signed the joint statement calling for a cease-fire because “obviously they have a close relationship with Haftar.”\\n\\nThe official also pointed to sensitivities surrounding Muslim Brotherhood political factions in Libya and asserted that Turkey and Qatar are “much more pro-Muslim Brotherhood and hence more closely aligned” with the Government of National Accord.\\n\\nA source outside the U.S. government but closely affiliated with Libyan political factions said the Muslim Brotherhood factor plays heavily into how key U.S. officials, including Mr. Trump and National Security Adviser John R. Bolton, view the situation.\\n\\n“There’s a disconnect between the way President Trump and Ambassador Bolton are looking at this and the way others, including Obama-era people at the State Department, are seeing this,” said the source, who added that the Trump-Bolton view is that Mr. al-Sarraj and the Tripoli government were effectively put into place by the former administration.\\n\\nThe idea, the source said, is that Mr. al-Sarraj and the Tripoli government are akin to Egyptian Muslim Brotherhood leader Mohammed Morsi, who was elected president after the 2011 revolution, and that Mr. Haftar is akin to Egyptian Gen. Abdel-Fattah el-Sissi, who ousted Mr. Morsi in a military coup in 2013 and classified the Muslim Brotherhood as a terrorist organization.\\n\\n“That’s how Trump sees it in Libya,” the source said.\\n\\nEnter the lobbyists\\n\\nAdding to the murkiness is the fact that the different factions in Libya have all signed on with powerful U.S. lobbyists.\\n\\nJustice Department foreign agent registration records show Mr. Haftar and his Libyan National Army are paying some $2 million to the Texas-based firm Linden Government Solutions to help with “international coalition building” and “general public relations.”\\n\\nCompany President Stephen Payne says in the filing that Mr. Haftar “is dedicated to stabilizing Libya, eradicating ISIS and al Qaeda, and ensuring free and fair elections for the first time in almost a decade.”\\n\\nOn the other side, the government in Tripoli hired the Washington-based firm Mercury to improve its image just days after the White House revealed Mr. Trump’s phone call and public endorsement of Mr. Haftar. Politico first reported the contract in May.\\n\\nMore recently, Prime Policy Group hosted Ahmed Omar Maiteeq, a top deputy in the al-Sarraj government, in Washington and represented him on a pro-bono basis in meeting U.S. officials.\\n\\nIt remains to been seen how the lobbying activity might affect the Trump administration’s policy or the situation on the ground in Libya.\\n\\nA source close to the government in Tripoli told The Times that “the whole goal is to promote a cease-fire and set the conditions so there can be a government of national unity.”\\n\\n“Our guys see a significant role for Haftar in a unified government, but obviously they also want to survive themselves,” said the source. “If they got a government together and Haftar was the head or the co-head, maybe he could bring the militias into the fold. Maybe there could be peace.”\\n\\nSign up for Daily Newsletters Manage Newsletters\\n\\nCopyright © 2019 The Washington Times, LLC. Click here for reprint permission.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7sIN-wiJRWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c11a48ff-e27d-4625-94f5-dee25a2adf19"
      },
      "source": [
        "article_text = article_text.split('\\n\\n')[1]\n",
        "print(article_text)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libya is spiraling toward its third civil war in less than a decade, creating a foreign policy crisis for the Trump administration in a strategic oil-rich corner of North Africa amid rising fears that jihadi terrorist groups are once again exploiting the power vacuum opened by renewed violence in recent months.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rsMBBMcv_nRM",
        "outputId": "8d41eb9c-4015-4f4e-b30f-d331fc346188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
        "\n",
        "# aim is to one hot encode everything \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(article_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  31\n",
            "txt_data_size :  312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aQygqc_CAWRA",
        "outputId": "c7adf999-8be0-4bed-8974-a151cd1b41cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'s': 0, 'f': 1, 'g': 2, 'm': 3, ',': 4, 'l': 5, 'i': 6, ' ': 7, 'o': 8, 'a': 9, 'r': 10, 'A': 11, '-': 12, 'w': 13, 'j': 14, 'e': 15, 't': 16, 'x': 17, 'h': 18, 'v': 19, 'p': 20, 'd': 21, '.': 22, 'b': 23, 'L': 24, 'T': 25, 'u': 26, 'y': 27, 'n': 28, 'c': 29, 'N': 30}\n",
            "----------------------------------------------------\n",
            "{0: 's', 1: 'f', 2: 'g', 3: 'm', 4: ',', 5: 'l', 6: 'i', 7: ' ', 8: 'o', 9: 'a', 10: 'r', 11: 'A', 12: '-', 13: 'w', 14: 'j', 15: 'e', 16: 't', 17: 'x', 18: 'h', 19: 'v', 20: 'p', 21: 'd', 22: '.', 23: 'b', 24: 'L', 25: 'T', 26: 'u', 27: 'y', 28: 'n', 29: 'c', 30: 'N'}\n",
            "----------------------------------------------------\n",
            "[24, 6, 23, 27, 9, 7, 6, 0, 7, 0, 20, 6, 10, 9, 5, 6, 28, 2, 7, 16, 8, 13, 9, 10, 21, 7, 6, 16, 0, 7, 16, 18, 6, 10, 21, 7, 29, 6, 19, 6, 5, 7, 13, 9, 10, 7, 6, 28, 7, 5, 15, 0, 0, 7, 16, 18, 9, 28, 7, 9, 7, 21, 15, 29, 9, 21, 15, 4, 7, 29, 10, 15, 9, 16, 6, 28, 2, 7, 9, 7, 1, 8, 10, 15, 6, 2, 28, 7, 20, 8, 5, 6, 29, 27, 7, 29, 10, 6, 0, 6, 0, 7, 1, 8, 10, 7, 16, 18, 15, 7, 25, 10, 26, 3, 20, 7, 9, 21, 3, 6, 28, 6, 0, 16, 10, 9, 16, 6, 8, 28, 7, 6, 28, 7, 9, 7, 0, 16, 10, 9, 16, 15, 2, 6, 29, 7, 8, 6, 5, 12, 10, 6, 29, 18, 7, 29, 8, 10, 28, 15, 10, 7, 8, 1, 7, 30, 8, 10, 16, 18, 7, 11, 1, 10, 6, 29, 9, 7, 9, 3, 6, 21, 7, 10, 6, 0, 6, 28, 2, 7, 1, 15, 9, 10, 0, 7, 16, 18, 9, 16, 7, 14, 6, 18, 9, 21, 6, 7, 16, 15, 10, 10, 8, 10, 6, 0, 16, 7, 2, 10, 8, 26, 20, 0, 7, 9, 10, 15, 7, 8, 28, 29, 15, 7, 9, 2, 9, 6, 28, 7, 15, 17, 20, 5, 8, 6, 16, 6, 28, 2, 7, 16, 18, 15, 7, 20, 8, 13, 15, 10, 7, 19, 9, 29, 26, 26, 3, 7, 8, 20, 15, 28, 15, 21, 7, 23, 27, 7, 10, 15, 28, 15, 13, 15, 21, 7, 19, 6, 8, 5, 15, 28, 29, 15, 7, 6, 28, 7, 10, 15, 29, 15, 28, 16, 7, 3, 8, 28, 16, 18, 0, 22]\n",
            "----------------------------------------------------\n",
            "data length :  312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bcpMSWDHFowT",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 1000\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 500  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bkqoN86qWaI4"
      },
      "source": [
        "#### Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "imfg_Ew0WdDL",
        "colab": {}
      },
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zm6qwNiqWdMe"
      },
      "source": [
        "#### Backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81qBiz_xWenI",
        "colab": {}
      },
      "source": [
        "def backprop(ps, inputs, hs, xs, targets):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r8sBvcdbWfhi"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iA4RM70LWgO_",
        "outputId": "a1d10e9d-3234-4d2e-c1f1-f946426e75f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 906.133428\n",
            "iter 100, loss: 79.407475\n",
            "iter 200, loss: 76.246420\n",
            "iter 300, loss: 73.501539\n",
            "iter 400, loss: 69.869432\n",
            "iter 500, loss: 75.195737\n",
            "iter 600, loss: 67.184838\n",
            "iter 700, loss: 60.725223\n",
            "iter 800, loss: 54.385651\n",
            "iter 900, loss: 54.204220\n",
            "CPU times: user 8min 14s, sys: 5min 23s, total: 13min 37s\n",
            "Wall time: 6min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tjh8Ip68WgYV"
      },
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HDCxDNPG68Hx",
        "colab": {}
      },
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1)) \n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) \n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1 \n",
        "        ixes.append(ix) # list\n",
        "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGVhl-Gxh6N6",
        "outputId": "e0c8b70b-fb50-4000-f4f8-a572539513db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "predict('T', 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " The h019, f \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xPsz-oefL1kP"
      },
      "source": [
        "Well... that's *vaguely* language-looking. Can you do better?"
      ]
    }
  ]
}